{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2021/04/25/hello-world/"},{"title":"math-demo","text":"数学公式demo$$E = mc^2$$ 行内公式的应用：$$a_i = a_{i-1} + a_{i-2} $$ .","link":"/2021/04/25/math-demo/"},{"title":"code-demo","text":"尝试使用code功能所有的编辑在 Typora 内完成。 123456def love(): while(True): print(&quot;睿睿子冲冲冲！&quot;)if __name__ == &quot;main&quot;: love()","link":"/2021/04/25/code-demo/"},{"title":"README","text":"欢迎来到嘉嘉小站Things you should know:弛宝冲冲冲！最爱睿宝！！！！","link":"/2021/04/25/README/"},{"title":"正则表达式（Regular expression）","text":"Python课上教到正则表达式，摸鱼摸狠了没听懂多少。自己跟着网上教程学了点基本语法，正好开个帖子记录一下以供以后查阅。以下内容根据RegexOne与Github的内容改写。 一个很好的的playground：regex101 字符正则表达式里的字符多数只能匹配他们本身。所有的字符和数字，都只能匹配自己，比如 1dog 就能匹配一个字符串，包含一个以’d’开头，’g’结尾，中间一个’o’的字符串。 练习写出一个正则表达式，可以完成下表中的任务。所有的练习解法都不唯一。 目标 文本 匹配 ab 匹配 abcd 匹配 abc 样例解1ab 数字数字0-9可以用\\d表示，其中\\为转义字符。如果要在表达式中用到\\，则需要再加上一个\\，也就是\\\\。 应当注意现在的表达式是与字符串中的任意位置匹配，而不是从第一个字符开始。 练习 目标 文本 匹配 abc123xyz 匹配 define”123” 匹配 var g=123; 样例解1123 点 “.”在正则表达式中，. 表示任意一个字符（字母，数字，空格等等）。同理，如果要匹配句号，那么在它前面加上一个转义符：\\. 例如，下面这个表达式 1c.t 表示匹配一个开头为”c”，结尾为”t”，中间字符任意的字符串，可以为”cat”, “cnt”, “c t”等等，但是它不能匹配”cnnt”, “ct”这种字符串。 练习 目标 文本 匹配 cat. 匹配 896. 匹配 ?=+. 忽略 abc1 样例解1...\\. or \\. 字符类字符类有时候 . 这个元字符适用的范围过大，我们需要缩小匹配的范围。比如说用于校验手机号时，我们不希望”abc-defg-hijk”是一组合法的输入。 在方括号中的字符称作字符类，表示可以匹配其中的任意一组字符。比如说 c[aeiou]t，表示匹配以c开头，t结尾，中间是aeiou中的任意一个字符的字符串。 [0123456789]，表示匹配任意一个数字 当需要匹配”[“与”]”本身时，在之前加上转义字符即可。字符类可以看成一个set，其中的元素顺序和个数都不重要。 字符类内和字符类外的字符规则有时会不同，比如 . 表示匹配任意一个字符，[.]表示匹配一个全角句号。 字符类的范围有时候字符类中的符号很多，一一列举出来很麻烦，于是引入字符类的范围，用-表示。比如 [a-z] 表示匹配从a到z的所有字符 [0-9] 表示匹配所有的数字字符 [0-9A-Fa-f] 表示匹配一个十六进制数字 [a-zA-Z\\-] 表示匹配所有字母或者短横线，注意在字符类中-需要加上转义符，在字符类外没有这个要求。 字符类的排除有时候从反面列举不需要的符号更加容易一些，这时候我们在方括号的开始处增加^. 比如 [^a-z] 表示匹配不是小写字母的字符 [^ \\^abc]表示匹配不是^或a或b或c的字符 练习 任务 文本 匹配 hog 匹配 bog 匹配 pog 忽略 dog 样例解答1[^d]og 转义字符类有些常用的匹配范围可以用转移字符表示。 \\d 表示匹配数字字符，等价于[0-9] \\w 表示匹配字母或数字或下划线，等价于[a-zA-Z0-9_]，常用于匹配英文文本 \\s 表示匹配一个空字符，空字符的定义包含空格，tab键，回车或者换行符 \\D 表示匹配不是数字的字符，等价于[^0-9] . \\W 表示匹配不是字母也不是数字，字母和下划线的字符，等价于[^0-9a-zA-Z_] \\S 表示匹配一个非空字符 练习匹配YYYY-MM-DD格式的字符，无需考虑月份和日期的合法性。 样例解1\\d\\d\\d\\d-\\d\\d-\\d\\d 接下来的内容会讲述如何简化这个表达式。 重复在字符后，可以用大括号内数字表示重复。一个数字表示重复次数，两个用逗号隔开的数字表示重复字数的范围。比如 a{3} 表示匹配含有三个连续的a的字符串，与aaa等价 a{1, 3} 表示匹配含a但不连续出现超过三次的字符串，也就是匹配a或aa或aaa [xyz]{5} 表示匹配五个字符，每个字符可以是x, y, z中的任意一个，比如xxxyz .{2, 6} 表示匹配长度为2-6的任意字符串 注意在字符类中，大括号没有特殊含义，[{}]表示匹配一个左大括号或者右大括号。a\\{2\\}表示匹配字符串”a{2}”。 重复次数可以是开区间 a{1,} 表示匹配a至少出现一次的字符串 .{0,} 表示匹配任意内容，即使是空字符串，这个表达式也会匹配全文并返回结果 练习T1 任务 文本 匹配 wazzzzzup 匹配 wazzzup 忽略 wazup T2使用正则表达式找到双引号，要求输入字符串可能包含任意个字符。 改进表达式，使得两个双引号中间不能含其它的双引号。 样例解T11waz{2,5}up T2&quot;.{0,}&quot; 改进后 &quot;[^&quot;{0,}]&quot; 关于重复的元字符 * 与{0,}相同。比如.*表示匹配任意内容 ? 与{0,1}相同。比如colou?r表示匹配color或者colour + 与{1,}相同。比如\\w+表示匹配一个单词，比如Account_name_1 如果需要匹配元字符本身，那么也是需要在它前面加上转移符号。 练习简化正则表达式： “.{0,}” x?x?x? y*y* 解答 “.*” x{0,3} y* 开始与结束到目前为止，我们的正则表达式都是部分匹配文本中的片段。有时候这并非我们所求。比如说要在日志文件中匹配”success”，我们不希望匹配到诸如”Error: unsuccessful operation”的片段。因此我们希望自己写的正则表达式越精确越好。 使表达式更加精确的一种方法是使用特定的元字符^与$描述一行的开始与结束。举个例子，上一段中，我们可以用 ^success 来匹配一行以success开头的文本。 注意^ 与[^ ] 不一样，有时容易造成混淆。 练习 任务 文本 匹配 Mission: successful 忽略 Last Mission: unsuccessful 忽略 Next Mission: successful upon capture of target 样例解答1^Mission: successful$ 捕获正则表达式不仅可以匹配文本，同时能够从中提取信息以供进一步处理。假如正则表达式是一个小型计算机程序，那么捕获子串就是它输出的一部分。在表达式中，我们用括号括起来需要捕获的文本。 比如你需要处理文件夹所有文件中的图片，可以用^(IMG\\d+\\\\.png)$ 捕获并提取完整文件名，但是如果你只想提取文件名不需要扩展名，那么可以使用 ^(IMG\\d+)\\.png$​ 捕获文件名。 如果有括号套括号的情况，那么捕获组对左括号计数。 假设有这样的正则表达式：(\\w+) had a ((\\w+) \\w+) 输入的内容是：I had a nice day 捕获组1：I 捕获组2：nice day 捕获组3：nice 练习T1 任务 文本 捕获 捕获 file_record_transcript.pdf file_record_transcript 捕获 file_07241999.pdf file_07241999 忽略 testfile_fake.pdf.tmp T2 任务 文本 捕获 捕获 Jan 1987 Jan 1987 1987 捕获 May 1969 May 1969 1969 捕获 Aug 2011 Aug 2011 2011 样例解T11^(file.+)\\.pdf$ T21(\\w+ (\\d+)) 更多内容之前遇到的元字符，包括星号*，加号+，数量范围{m,n}等，都可以用在捕获组之中或用来修饰捕获组。 比如说，我们要判断座机电话合法性，不确定是否会在开头加上区号，正确的表达式会用(\\d{3})?来判断区号是否存在。 选择匹配为了表示匹配的不同选择，可以使用 | 符号。比如”Buy more (milk|bread|cookies).” 表示匹配”Buy more milk.” “Buy more bread.” “Buy more cookies.”中的一个。 我们可以在选项中使用元字符，比如([cb]ats*|[dh]ogs?)会匹配cats或者bats，或者dogs或hogs。 结束目前接触到的基本语法就这些，至于正则表达式在python中的使用方法，等以后用到再总结吧。","link":"/2021/05/09/regex/"},{"title":"用Torch搭建手写字体识别","text":"最近在学习使用Torch，记录一下学习经历。为了熟悉基本的操作和流程，于是torch搭了一个简单的手写字体识别的模型。 加载数据pytorch有两种数据处理形式：torch.utils.data.DataLoader和torch.utils.data.Dataset. Dataset存有一些常用的数据集，数据以sample和对应的label的形式存储，DataLoader以可迭代对象存储数据集。在这里，MNIST已经由API提供，直接用就可以了。 首先加载必要的库。 12345678910from __future__ import print_functionimport torchimport torch.nn as nnimport torch.optim as optimfrom torchvision import datasets, transformsfrom torch.utils.data import DataLoader# 为保证可复现，手动设置种子manual_seed = 42torch.manual_seed(manual_seed) 接着加载数据集。 123456789101112131415161718def get_dataloader(batch_size=64): # transforms表示对图片的预处理方式，这里为将数据转化为Tensor transform = transforms.ToTensor() # 下载数据 train_data = datasets.MNIST('../data', train=True, download=True, transform=transform) test_data = datasets.MNIST('../data', train=False, download=True, transform=transform) # 创建DataLoader对象，这将数据集处理成可迭代对象，从而更方便的进行批处理 train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4) test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4) for X, y in test_loader: print(&quot;Shape of X:&quot;, X.shape) print(&quot;Shape of y:&quot;, y.shape) break return train_loader, test_loader 运行该函数，得到输出： 12Shape of X: torch.Size([64, 1, 28, 28])Shape of y: torch.Size([64]) torch.int64 建立模型我们需要自定义一个用于训练的神经网络，我们需要在__init__方法里定义网络结构，在forward方法里说明数据在神经网络中是如何传输的。简明起见只用了很浅的网络。 1234567891011121314151617181920212223# 尽量使用gpudevice = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;# 定义模型class MyModel(nn.Module): def __init__(self): super(MyModel, self).__init__() self.main = nn.Sequential( nn.Flatten(), nn.Linear(28 * 28, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU(), nn.Linear(512, 10) ) def forward(self, input): return self.main(input)def get_model(): model = MyModel().to(device) return model 优化模型参数为了训练模型，我们需要设置损失函数和优化器。由于这是个分类问题，损失函数选择CrossEntropyLoss, 优化器选择SGD。 12optimizer = optim.SGD(model.parameters(), lr=5e-4)loss_function = nn.CrossEntropyLoss() 在一个epoch里，模型对训练集做预测，算得损失函数后通过反向传播算得各个参数的梯度，然后更新参数。 12345678910111213141516def train(dataloader, model, loss_function, optimizer): model.train() # 插眼 for batch_idx, (data, target) in enumerate(train_loader): data, target = data.to(device), target.to(device) # 计算预测损失 output = model(data) loss = loss_function(output, target) # 反向传播 optimizer.zero_grad() loss.backward() optimizer.step() # 更新参数 if batch_idx % 100 == 0: print(&quot;[{:&gt;5d}/{:&gt;5d}]\\tLoss: {:.6f}&quot;.format(epoch, batch_idx * len(data), len(train_loader.dataset), loss.item())) 接下来我们在测试集上对模型的表现进行检验。 1234567891011121314151617def test(dataloader, model, loss_function): model.eval() # 插眼 test_loss, correct = 0, 0 with torch.no_grad(): for data, target in test_loader: data, target = data.to(device), target.to(device) output = model(data) # 计算损失函数和预测正确数量 test_loss += loss_function(output, target).item() correct += (output.argmax(1) == target).type(torch.float).sum().item() test_loss /= len(test_loader) print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format( test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) 值得一提的是，在训练和测验函数中，有model.train()和model.test()一行。查阅资料得知，模型中诸如Batch Norm, Dropout层这些在训练和测试时运行模式不一样，需要指定模式，比如model.eval()会将BN层的值固定住不取平均，Dropout层不发挥作用，model.train()反之。 搭积木基本的流程已经搭建好了，拼一块就行。 1234567891011train_loader, test_loader = get_dataloader()model = get_model()epochs = 32for epoch in range(epochs): print(&quot;Epoch {}&quot;.format(epoch + 1)) print(&quot;-&quot; * 30) train(train_loader, model, loss_function, optimizer) test(test_loader, model, loss_function) print(&quot;Done!&quot;) 运行，截取最后一个epoch输出如下 123456789101112131415Epoch 32------------------------------[ 0/60000] Loss: 0.339374[ 6400/60000] Loss: 0.416344[12800/60000] Loss: 0.342802[19200/60000] Loss: 0.491385[25600/60000] Loss: 0.413908[32000/60000] Loss: 0.349018[38400/60000] Loss: 0.593918[44800/60000] Loss: 0.474432[51200/60000] Loss: 0.355993[57600/60000] Loss: 0.284024Test set: Average loss: 0.4354, Accuracy: 8835.0/10000 (88%) 我们模型在测试集上的表现为88%。下面写个函数看一看误分类的样本。 1234567891011121314def find_wrong(modal, test_loader): wrong_sample = [] wrong_label = [] modal.eval() with torch.no_grad(): for data, target in test_loader: data, target = data.to(device), target.to(device) pred = model(data).argmax(1) for index, (i, j) in enumerate(zip(pred, target)): if not torch.equal(i, j): wrong_sample.append(data[index]) wrong_label.append(target[index]) return wrong_sample, wrong_label 12345678910import matplotlib.pyplot as pltwsample, wlabel = find_wrong(model, test_loader)for i in range(1, 10): plt.subplot(3, 3, i) plt.imshow(wsample[i][0].cpu()) plt.title(wlabel[i].cpu().item()) plt.xticks([]) plt.yticks([])plt.show() 画出错误分类的前9个样本，看出来有一定的迷惑度。 总结搭建了一个简单的模型，在mnist上训练，最终在测试集上取得88%的准确度。如果将网络模型调的更复杂些，epoch增加些，效果会好很多。","link":"/2021/09/25/Torch-mnist/"}],"tags":[{"name":"demo","slug":"demo","link":"/tags/demo/"},{"name":"tutorial","slug":"tutorial","link":"/tags/tutorial/"},{"name":"regex","slug":"regex","link":"/tags/regex/"}],"categories":[]}