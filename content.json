{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2021/04/25/hello-world/"},{"title":"math-demo","text":"数学公式demo$$E = mc^2$$ 行内公式的应用：$$a_i = a_{i-1} + a_{i-2} $$ .","link":"/2021/04/25/math-demo/"},{"title":"code-demo","text":"尝试使用code功能所有的编辑在 Typora 内完成。 123456def love(): while(True): print(&quot;睿睿子冲冲冲！&quot;)if __name__ == &quot;main&quot;: love()","link":"/2021/04/25/code-demo/"},{"title":"README","text":"欢迎来到嘉嘉小站Things you should know:弛宝冲冲冲！最爱睿宝！！！！","link":"/2021/04/25/README/"},{"title":"正则表达式（Regular expression）","text":"Python课上教到正则表达式，摸鱼摸狠了没听懂多少。自己跟着网上教程学了点基本语法，正好开个帖子记录一下以供以后查阅。以下内容根据RegexOne与Github的内容改写。 一个很好的的playground：regex101 字符正则表达式里的字符多数只能匹配他们本身。所有的字符和数字，都只能匹配自己，比如 1dog 就能匹配一个字符串，包含一个以’d’开头，’g’结尾，中间一个’o’的字符串。 练习写出一个正则表达式，可以完成下表中的任务。所有的练习解法都不唯一。 目标 文本 匹配 ab 匹配 abcd 匹配 abc 样例解1ab 数字数字0-9可以用\\d表示，其中\\为转义字符。如果要在表达式中用到\\，则需要再加上一个\\，也就是\\\\。 应当注意现在的表达式是与字符串中的任意位置匹配，而不是从第一个字符开始。 练习 目标 文本 匹配 abc123xyz 匹配 define”123” 匹配 var g=123; 样例解1123 点 “.”在正则表达式中，. 表示任意一个字符（字母，数字，空格等等）。同理，如果要匹配句号，那么在它前面加上一个转义符：\\. 例如，下面这个表达式 1c.t 表示匹配一个开头为”c”，结尾为”t”，中间字符任意的字符串，可以为”cat”, “cnt”, “c t”等等，但是它不能匹配”cnnt”, “ct”这种字符串。 练习 目标 文本 匹配 cat. 匹配 896. 匹配 ?=+. 忽略 abc1 样例解1...\\. or \\. 字符类字符类有时候 . 这个元字符适用的范围过大，我们需要缩小匹配的范围。比如说用于校验手机号时，我们不希望”abc-defg-hijk”是一组合法的输入。 在方括号中的字符称作字符类，表示可以匹配其中的任意一组字符。比如说 c[aeiou]t，表示匹配以c开头，t结尾，中间是aeiou中的任意一个字符的字符串。 [0123456789]，表示匹配任意一个数字 当需要匹配”[“与”]”本身时，在之前加上转义字符即可。字符类可以看成一个set，其中的元素顺序和个数都不重要。 字符类内和字符类外的字符规则有时会不同，比如 . 表示匹配任意一个字符，[.]表示匹配一个全角句号。 字符类的范围有时候字符类中的符号很多，一一列举出来很麻烦，于是引入字符类的范围，用-表示。比如 [a-z] 表示匹配从a到z的所有字符 [0-9] 表示匹配所有的数字字符 [0-9A-Fa-f] 表示匹配一个十六进制数字 [a-zA-Z\\-] 表示匹配所有字母或者短横线，注意在字符类中-需要加上转义符，在字符类外没有这个要求。 字符类的排除有时候从反面列举不需要的符号更加容易一些，这时候我们在方括号的开始处增加^. 比如 [^a-z] 表示匹配不是小写字母的字符 [^ \\^abc]表示匹配不是^或a或b或c的字符 练习 任务 文本 匹配 hog 匹配 bog 匹配 pog 忽略 dog 样例解答1[^d]og 转义字符类有些常用的匹配范围可以用转移字符表示。 \\d 表示匹配数字字符，等价于[0-9] \\w 表示匹配字母或数字或下划线，等价于[a-zA-Z0-9_]，常用于匹配英文文本 \\s 表示匹配一个空字符，空字符的定义包含空格，tab键，回车或者换行符 \\D 表示匹配不是数字的字符，等价于[^0-9] . \\W 表示匹配不是字母也不是数字，字母和下划线的字符，等价于[^0-9a-zA-Z_] \\S 表示匹配一个非空字符 练习匹配YYYY-MM-DD格式的字符，无需考虑月份和日期的合法性。 样例解1\\d\\d\\d\\d-\\d\\d-\\d\\d 接下来的内容会讲述如何简化这个表达式。 重复在字符后，可以用大括号内数字表示重复。一个数字表示重复次数，两个用逗号隔开的数字表示重复字数的范围。比如 a{3} 表示匹配含有三个连续的a的字符串，与aaa等价 a{1, 3} 表示匹配含a但不连续出现超过三次的字符串，也就是匹配a或aa或aaa [xyz]{5} 表示匹配五个字符，每个字符可以是x, y, z中的任意一个，比如xxxyz .{2, 6} 表示匹配长度为2-6的任意字符串 注意在字符类中，大括号没有特殊含义，[{}]表示匹配一个左大括号或者右大括号。a\\{2\\}表示匹配字符串”a{2}”。 重复次数可以是开区间 a{1,} 表示匹配a至少出现一次的字符串 .{0,} 表示匹配任意内容，即使是空字符串，这个表达式也会匹配全文并返回结果 练习T1 任务 文本 匹配 wazzzzzup 匹配 wazzzup 忽略 wazup T2使用正则表达式找到双引号，要求输入字符串可能包含任意个字符。 改进表达式，使得两个双引号中间不能含其它的双引号。 样例解T11waz{2,5}up T2&quot;.{0,}&quot; 改进后 &quot;[^&quot;{0,}]&quot; 关于重复的元字符 * 与{0,}相同。比如.*表示匹配任意内容 ? 与{0,1}相同。比如colou?r表示匹配color或者colour + 与{1,}相同。比如\\w+表示匹配一个单词，比如Account_name_1 如果需要匹配元字符本身，那么也是需要在它前面加上转移符号。 练习简化正则表达式： “.{0,}” x?x?x? y*y* 解答 “.*” x{0,3} y* 开始与结束到目前为止，我们的正则表达式都是部分匹配文本中的片段。有时候这并非我们所求。比如说要在日志文件中匹配”success”，我们不希望匹配到诸如”Error: unsuccessful operation”的片段。因此我们希望自己写的正则表达式越精确越好。 使表达式更加精确的一种方法是使用特定的元字符^与$描述一行的开始与结束。举个例子，上一段中，我们可以用 ^success 来匹配一行以success开头的文本。 注意^ 与[^ ] 不一样，有时容易造成混淆。 练习 任务 文本 匹配 Mission: successful 忽略 Last Mission: unsuccessful 忽略 Next Mission: successful upon capture of target 样例解答1^Mission: successful$ 捕获正则表达式不仅可以匹配文本，同时能够从中提取信息以供进一步处理。假如正则表达式是一个小型计算机程序，那么捕获子串就是它输出的一部分。在表达式中，我们用括号括起来需要捕获的文本。 比如你需要处理文件夹所有文件中的图片，可以用^(IMG\\d+\\\\.png)$ 捕获并提取完整文件名，但是如果你只想提取文件名不需要扩展名，那么可以使用 ^(IMG\\d+)\\.png$​ 捕获文件名。 如果有括号套括号的情况，那么捕获组对左括号计数。 假设有这样的正则表达式：(\\w+) had a ((\\w+) \\w+) 输入的内容是：I had a nice day 捕获组1：I 捕获组2：nice day 捕获组3：nice 练习T1 任务 文本 捕获 捕获 file_record_transcript.pdf file_record_transcript 捕获 file_07241999.pdf file_07241999 忽略 testfile_fake.pdf.tmp T2 任务 文本 捕获 捕获 Jan 1987 Jan 1987 1987 捕获 May 1969 May 1969 1969 捕获 Aug 2011 Aug 2011 2011 样例解T11^(file.+)\\.pdf$ T21(\\w+ (\\d+)) 更多内容之前遇到的元字符，包括星号*，加号+，数量范围{m,n}等，都可以用在捕获组之中或用来修饰捕获组。 比如说，我们要判断座机电话合法性，不确定是否会在开头加上区号，正确的表达式会用(\\d{3})?来判断区号是否存在。 选择匹配为了表示匹配的不同选择，可以使用 | 符号。比如”Buy more (milk|bread|cookies).” 表示匹配”Buy more milk.” “Buy more bread.” “Buy more cookies.”中的一个。 我们可以在选项中使用元字符，比如([cb]ats*|[dh]ogs?)会匹配cats或者bats，或者dogs或hogs。 结束目前接触到的基本语法就这些，至于正则表达式在python中的使用方法，等以后用到再总结吧。","link":"/2021/05/09/regex/"},{"title":"用Torch搭建手写字体识别模型","text":"最近在学习使用Torch，记录一下学习经历。为了熟悉基本的操作和流程，于是torch搭了一个简单的手写字体识别的模型。 加载数据pytorch有两种数据处理形式：torch.utils.data.DataLoader和torch.utils.data.Dataset. Dataset存有一些常用的数据集，数据以sample和对应的label的形式存储，DataLoader以可迭代对象存储数据集。在这里，MNIST已经由API提供，直接用就可以了。 首先加载必要的库。 12345678910from __future__ import print_functionimport torchimport torch.nn as nnimport torch.optim as optimfrom torchvision import datasets, transformsfrom torch.utils.data import DataLoader# 为保证可复现，手动设置种子manual_seed = 42torch.manual_seed(manual_seed) 接着加载数据集。 123456789101112131415161718def get_dataloader(batch_size=64): # transforms表示对图片的预处理方式，这里为将数据转化为Tensor transform = transforms.ToTensor() # 下载数据 train_data = datasets.MNIST('../data', train=True, download=True, transform=transform) test_data = datasets.MNIST('../data', train=False, download=True, transform=transform) # 创建DataLoader对象，这将数据集处理成可迭代对象，从而更方便的进行批处理 train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4) test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4) for X, y in test_loader: print(&quot;Shape of X:&quot;, X.shape) print(&quot;Shape of y:&quot;, y.shape) break return train_loader, test_loader 运行该函数，得到输出： 12Shape of X: torch.Size([64, 1, 28, 28])Shape of y: torch.Size([64]) torch.int64 建立模型我们需要自定义一个用于训练的神经网络，我们需要在__init__方法里定义网络结构，在forward方法里说明数据在神经网络中是如何传输的。简明起见只用了很浅的网络。 1234567891011121314151617181920212223# 尽量使用gpudevice = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;# 定义模型class MyModel(nn.Module): def __init__(self): super(MyModel, self).__init__() self.main = nn.Sequential( nn.Flatten(), nn.Linear(28 * 28, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU(), nn.Linear(512, 10) ) def forward(self, input): return self.main(input)def get_model(): model = MyModel().to(device) return model 优化模型参数为了训练模型，我们需要设置损失函数和优化器。由于这是个分类问题，损失函数选择CrossEntropyLoss, 优化器选择SGD。 12optimizer = optim.SGD(model.parameters(), lr=5e-4)loss_function = nn.CrossEntropyLoss() 在一个epoch里，模型对训练集做预测，算得损失函数后通过反向传播算得各个参数的梯度，然后更新参数。 12345678910111213141516def train(dataloader, model, loss_function, optimizer): model.train() # 插眼 for batch_idx, (data, target) in enumerate(train_loader): data, target = data.to(device), target.to(device) # 计算预测损失 output = model(data) loss = loss_function(output, target) # 反向传播 optimizer.zero_grad() loss.backward() optimizer.step() # 更新参数 if batch_idx % 100 == 0: print(&quot;[{:&gt;5d}/{:&gt;5d}]\\tLoss: {:.6f}&quot;.format(epoch, batch_idx * len(data), len(train_loader.dataset), loss.item())) 接下来我们在测试集上对模型的表现进行检验。 1234567891011121314151617def test(dataloader, model, loss_function): model.eval() # 插眼 test_loss, correct = 0, 0 with torch.no_grad(): for data, target in test_loader: data, target = data.to(device), target.to(device) output = model(data) # 计算损失函数和预测正确数量 test_loss += loss_function(output, target).item() correct += (output.argmax(1) == target).type(torch.float).sum().item() test_loss /= len(test_loader) print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format( test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) 值得一提的是，在训练和测验函数中，有model.train()和model.test()一行。查阅资料得知，模型中诸如Batch Norm, Dropout层这些在训练和测试时运行模式不一样，需要指定模式，比如model.eval()会将BN层的值固定住不取平均，Dropout层不发挥作用，model.train()反之。 搭积木基本的流程已经搭建好了，拼一块就行。 1234567891011train_loader, test_loader = get_dataloader()model = get_model()epochs = 32for epoch in range(epochs): print(&quot;Epoch {}&quot;.format(epoch + 1)) print(&quot;-&quot; * 30) train(train_loader, model, loss_function, optimizer) test(test_loader, model, loss_function) print(&quot;Done!&quot;) 运行，截取最后一个epoch输出如下 123456789101112131415Epoch 32------------------------------[ 0/60000] Loss: 0.339374[ 6400/60000] Loss: 0.416344[12800/60000] Loss: 0.342802[19200/60000] Loss: 0.491385[25600/60000] Loss: 0.413908[32000/60000] Loss: 0.349018[38400/60000] Loss: 0.593918[44800/60000] Loss: 0.474432[51200/60000] Loss: 0.355993[57600/60000] Loss: 0.284024Test set: Average loss: 0.4354, Accuracy: 8835.0/10000 (88%) 我们模型在测试集上的表现为88%。下面写个函数看一看误分类的样本。 1234567891011121314def find_wrong(modal, test_loader): wrong_sample = [] wrong_label = [] modal.eval() with torch.no_grad(): for data, target in test_loader: data, target = data.to(device), target.to(device) pred = model(data).argmax(1) for index, (i, j) in enumerate(zip(pred, target)): if not torch.equal(i, j): wrong_sample.append(data[index]) wrong_label.append(target[index]) return wrong_sample, wrong_label 12345678910import matplotlib.pyplot as pltwsample, wlabel = find_wrong(model, test_loader)for i in range(1, 10): plt.subplot(3, 3, i) plt.imshow(wsample[i][0].cpu()) plt.title(wlabel[i].cpu().item()) plt.xticks([]) plt.yticks([])plt.show() 画出错误分类的前9个样本，看出来有一定的迷惑度。 总结搭建了一个简单的模型，在mnist上训练，最终在测试集上取得88%的准确度。如果将网络模型调的更复杂些，epoch增加些，效果会好很多。","link":"/2021/09/25/Torch-mnist/"},{"title":"GAN原理以及代码实现","text":"GAN（Generative Adversarial Nets）是生成模型的一种，其训练是在对抗博弈中进行的。GAN包含两个主要结构：生成器G（Generator）用于捕获数据分布规律并生成新的数据，判别器D（Discriminator）用于判断样本来自训练集还是生成器。以生成图片为例简述GAN的基本原理。 基本原理生成器生成器G接收一个随机的噪声z，通过这个噪声生成图片，记为G(z)。训练的目标是使得生成的图片尽可能的与数据集相似，从而骗过判别器G。 这里输入的向量我们将其视为携带输出的某些信息，比如说手写数字为数字几，手写的潦草程度等等。由于这里我们对于输出的具体信息不做要求，只要求其能够最大程度与真实数据相似（能骗过判别器）即可。所以我们使用随机生成的向量来作为输入即可，这里面的随机输入最好是满足常见分布比如均值分布，高斯分布等。 判别器判别器D接受一张图片x，输出标量D(x)代表x为真实图片的概率，1代表100%真实; 0代表不可能为真实图片。D为通常的网络结构，可以是Perceptron、CNN等等。 损失函数论文中的损失函数定义如下： $$ \\min_G \\max_D V(D,G)=\\mathbb{E}_{\\boldsymbol{x}\\sim p_{data}(\\boldsymbol{x})}[\\log D(x)] + \\mathbb{E}_{\\boldsymbol{z}\\sim p_{z}(\\boldsymbol{z})}[\\log (1-D(G(z)))] $$ 该函数由两部分构成，第一部分针对真实图片，第二部分针对输入网络的噪声。函数的目标是一个minimax函数，我们分别对其进行解释。 对于真实图片，$D(x)$代表判别器认为其是真实图片的概率; 对于噪声z，$D(G(z))$代表判别器认为生成器生成的图片是真实图片的概率。 对于判别器D，其需要尽可能的提高分辨能力，所以其训练目标为最大$D(x)$，最小$D(G(z))$，综合起来即求$\\max_D V(D)$ 对于生成器G，其需要自己生成的图片尽可能的真实，所以其训练目标是最大化$D(G(z))$，也即$min_G V(G)$ 可以预想到，在理想情况下，G生成的图片可以以假乱真，$D(G(z))=0.5$。 训练方法实际训练方法结合了Goodfellow论文中的算法以及ganhacks中的一些tricks，即将真实数据和生成数据分不同的mini-batch训练，将G的目标调整为$\\max\\log D(G(z))$ 初始化生成器G和判别器D的参数$\\theta_G,\\theta_D$ 从数据集中采样$m$个样本 ${x^{(1)}, x^{(2)}, \\cdots, x^{(m)}}$ ，计算损失$\\log(D(x))$，得到D参数的梯度。 从噪声中采样$m$个噪声样本 ${z^{(1)}, z^{(2)}, \\cdots, z^{(m)}}$ ，通过生成器得到$m$个生成样本 ${x^{(1)}, x^{(2)}, \\cdots, x^{(m)}}$ 。计算损失$log(1-D(G(z)))$，得到D参数的梯度，将两次的梯度积累在一起并反向传播。 重复2-3步$k$次训练判别器D 用训练得到的判别器D对生成器G的输出图像做分类，计算$\\log D(G(z))$求梯度并反向传播。 第2-5步为1个epoch，训练时不断重复。 代码实现以CIFAR-10为数据集，实现DCGAN网络。 首先导入一些必要的库。 123456789101112import torchimport torch.nn as nnimport torch.backends.cudnn as cudnnimport torch.optim as optimfrom torch.utils.data import Datasetimport torchvision.datasets as dsetimport torchvision.transforms as transformsimport torchvision.utils as vutilsfrom IPython.display import HTMLimport matplotlib.pyplot as plt 定义一些超参数后面使用 12345678910111213141516171819image_size = 64nz = 100lr = 0.0002ngf = 64ndf = 64num_epochs = 16batch_size = 128ngpu = 1beta1 = 0.5device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) 准备数据数据我选择的是从Google Drive下载到本地然后解压到data文件夹中，用ImageFolder方法加载数据。使用Torch自带的torchvision.datasets.CelebA会报错，查资料发现这个bug大家都有，遂放弃。 12345678910111213141516171819202122# 加载数据dataroot = &quot;data&quot;dataset = dset.ImageFolder(root=dataroot, transform=transforms.Compose([ transforms.Resize(image_size), transforms.CenterCrop(image_size), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ]))dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=12)# 确定设备device = torch.device(&quot;cuda:0&quot; if (torch.cuda.is_available() and ngpu &gt; 0) else &quot;cpu&quot;)# 为方便起见，我们可视化一些数据real_batch = next(iter(dataloader))plt.figure(figsize=(8,8))plt.axis(&quot;off&quot;)plt.title(&quot;Training Images&quot;)plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0))) 得到的图像如下 定义网络生成器首先定义生成器结构，参考DCGAN文献中的结构如图。z为从噪声中采样得到的100维向量，经过几次上采样，得到一个$3\\times 64\\times 64$的Tensor，也就是生成的RGB图像。 代码如下，为了节省算力，将Channel数都减半了。 123456789101112131415161718192021222324252627282930class Generator(nn.Module): def __init__(self): super(Generator, self).__init__() self.main = nn.Sequential( # ngf设置为64 nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False), #除最后一个ConvTranspose层，都有一个BatchNorm层 nn.BatchNorm2d(ngf * 8), nn.ReLU(True), nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf * 4), nn.ReLU(True), nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf * 2), nn.ReLU(True), nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf), nn.ReLU(True), nn.ConvTranspose2d(ngf, 3, 4, 2, 1, bias=False), nn.Tanh() ) def forward(self, x): return self.main(x) G = Generator().to(device) 判别器判别器D负责判断输入的图像是生成器生成的还是真实图像。它接收一个$3\\times 64\\times 64$的输入图像，经过一系列卷积操作输出概率。论文中提到最好用strided convolution而非用pooling layer下采样。该网络结构借鉴于PyTorch官网。 1234567891011121314151617181920212223242526272829class Discriminator(nn.Module): def __init__(self): super(Discriminator, self).__init__() self.main = nn.Sequential( # ndf设置为64 nn.Conv2d(3, ndf, 4, 2, 1, bias=False), nn.LeakyReLU(0.2, inplace=True), # 尺寸 (ndf) x 32 x 32 nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 2), nn.LeakyReLU(0.2, inplace=True), # 尺寸 (ndf*2) x 16 x 16 nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 4), nn.LeakyReLU(0.2, inplace=True), # 尺寸 (ndf*4) x 8 x 8 nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 8), nn.LeakyReLU(0.2, inplace=True), # 尺寸 (ndf*8) x 4 x 4 nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False), nn.Sigmoid() ) def forward(self, x): return self.main(x)D = Discriminator().to(device) 初始器论文作者提出网络初始参数采样自平均值0，标准差0.02的高斯分布。 123456789101112def init_weights(net): classname = net.__class__.__name__ if classname.find('Conv') != -1: nn.init.normal_(net.weight.data, 0.0, 0.02) elif classname.find('BatchNorm') != -1: nn.init.normal_(net.weight.data, 1.0, 0.02) nn.init.constant_(net.bias.data, 0)G.apply(init_weights)D.apply(init_weights)print(G)print(D) 输出： 1234567891011121314151617181920212223242526272829303132333435Generator( (main): Sequential( (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU(inplace=True) (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): ReLU(inplace=True) (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (11): ReLU(inplace=True) (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (13): Tanh() ))Discriminator( (main): Sequential( (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): LeakyReLU(negative_slope=0.2, inplace=True) (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (4): LeakyReLU(negative_slope=0.2, inplace=True) (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): LeakyReLU(negative_slope=0.2, inplace=True) (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (10): LeakyReLU(negative_slope=0.2, inplace=True) (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False) (12): Sigmoid() )) 损失函数和优化器损失函数选择交叉熵，在PyTorch中定义为 $$ l = -[y\\log(x) + (1-y)\\log(1-x)] $$ 这个函数与目标函数形式类似，我们可以通过设定$y$来选择使用损失函数的$\\log(x)$部分还是$\\log(1-x)$部分。 我们将真实图像的标签设置为1，生成图像标签设置为0。为D与G分别定义一个优化器，均为Adam，学习率0.0002，beta1=0.5 我们在每个epoch结束后向生成器输入一个固定的噪声，观察它的输出随着epoch的增加的变化。 12345678910# 损失函数criterion = nn.BCELoss()# 固定的噪声fixed_noise = torch.randn(64, nz, 1, 1, device=device)real_label = 1.fake_label = 0.# 优化器optimizerD = optim.Adam(D.parameters(), lr=lr, betas=(beta1, 0.999))optimizerG = optim.Adam(G.parameters(), lr=lr, betas=(beta1, 0.999)) 训练训练过程如原理部分所提到，代码只是将其实现了一遍。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# 用于记录训练结果img_list = []G_losses = []D_losses = []iters = 0print(&quot;Starting Training Loop...&quot;)for epoch in range(num_epochs): for i, data in enumerate(dataloader, 0): # Step 1: Update D network: maximize log(D(x)) + log(1 - D(G(z))) ## 用全部为真实图片的batch D.zero_grad() # 加载进gpu data_gpu = data[0].to(device) b_size = data_gpu.size(0) # 所有的图片标签均为1, 计算的损失函数为-log(D(x)) label = torch.full((b_size,), real_label, dtype=torch.float, device=device) # 将数据forward pass output = D(data_gpu).view(-1) # 计算loss errD_real = criterion(output, label) # 计算梯度 errD_real.backward() D_x = output.mean().item() ## 用全部是生成图片的batch # 生成假图片 noise = torch.randn(b_size, nz, 1, 1, device=device) fake = G(noise) # 将标签改为0，计算的损失函数为-log(1-D(G(z))) label.fill_(fake_label) # 一定要加fake.detach()，不然第二步无法反向传播 output = D(fake.detach()).view(-1) # 计算损失函数 errD_fake = criterion(output, label) # 计算梯度，与上个部分算得的梯度相加 errD_fake.backward() D_G_z1 = output.mean().item() # 计算D的总损失 errD = errD_real + errD_fake optimizerD.step() # Step 2: Update G network: maximize log(D(G(z))) G.zero_grad() label.fill_(real_label) # label填1,这样计算的损失函数为-log(D(G(z))) # 将生成器生成的图片输入分别器，得到输出 output = D(fake).view(-1) # 计算生成器的损失 errG = criterion(output, label) # 计算梯度 errG.backward() D_G_z2 = output.mean().item() optimizerG.step() # 输出训练情况 if i % 100 == 0: print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f' % (epoch, num_epochs, i, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2)) # 储存损失函数的值用于画图 G_losses.append(errG.item()) D_losses.append(errD.item()) # 观察生成器在给定输入下的输出随着epoch数增加的变化 if (iters % 1500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)): with torch.no_grad(): fake = G(fixed_noise).detach().cpu() img_list.append(vutils.make_grid(fake, padding=2, normalize=True)) iters += 1 截取部分输出如下 123456789101112131415161718192021Starting Training Loop...[0/16][0/1344] Loss_D: 1.9634 Loss_G: 4.5095 D(x): 0.4211 D(G(z)): 0.5242 / 0.0159[0/16][100/1344] Loss_D: 0.3319 Loss_G: 6.5696 D(x): 0.9065 D(G(z)): 0.1146 / 0.0058[0/16][200/1344] Loss_D: 0.8147 Loss_G: 8.3612 D(x): 0.9388 D(G(z)): 0.4045 / 0.0008[0/16][300/1344] Loss_D: 1.3598 Loss_G: 7.1813 D(x): 0.9571 D(G(z)): 0.6396 / 0.0034[0/16][400/1344] Loss_D: 1.1145 Loss_G: 7.4710 D(x): 0.9355 D(G(z)): 0.5557 / 0.0025[0/16][500/1344] Loss_D: 0.4718 Loss_G: 4.6667 D(x): 0.7850 D(G(z)): 0.0928 / 0.0221[0/16][600/1344] Loss_D: 0.4587 Loss_G: 3.8398 D(x): 0.8438 D(G(z)): 0.1818 / 0.0434[0/16][700/1344] Loss_D: 0.4853 Loss_G: 4.4762 D(x): 0.8795 D(G(z)): 0.2440 / 0.0224[0/16][800/1344] Loss_D: 0.4169 Loss_G: 3.6308 D(x): 0.7731 D(G(z)): 0.0740 / 0.0469[0/16][900/1344] Loss_D: 0.1671 Loss_G: 5.2397 D(x): 0.8985 D(G(z)): 0.0384 / 0.0102[0/16][1000/1344] Loss_D: 0.5116 Loss_G: 4.0730 D(x): 0.7616 D(G(z)): 0.1442 / 0.0312[0/16][1100/1344] Loss_D: 0.6857 Loss_G: 4.2473 D(x): 0.6337 D(G(z)): 0.0170 / 0.0272[0/16][1200/1344] Loss_D: 0.4163 Loss_G: 4.2029 D(x): 0.7515 D(G(z)): 0.0479 / 0.0308[0/16][1300/1344] Loss_D: 0.5726 Loss_G: 3.6670 D(x): 0.7889 D(G(z)): 0.2219 / 0.0416[1/16][0/1344] Loss_D: 0.7348 Loss_G: 6.1066 D(x): 0.9219 D(G(z)): 0.4060 / 0.0045...[15/16][1000/1344] Loss_D: 0.2496 Loss_G: 3.4040 D(x): 0.8612 D(G(z)): 0.0686 / 0.0578[15/16][1100/1344] Loss_D: 0.8923 Loss_G: 1.8258 D(x): 0.5667 D(G(z)): 0.1154 / 0.2363[15/16][1200/1344] Loss_D: 0.2764 Loss_G: 4.7261 D(x): 0.9499 D(G(z)): 0.1727 / 0.0137[15/16][1300/1344] Loss_D: 0.6088 Loss_G: 5.4057 D(x): 0.9769 D(G(z)): 0.3796 / 0.0097 训练结果看见结果初步感觉训练结果应该不对劲，理想状态下D(x)应当趋向于0.5，即无法分辨真实与虚假图像。 首先画出D和G的loss随着训练的改变。 12345678plt.figure(figsize=(20,10))plt.title(&quot;Generator and Discriminator Loss During Training&quot;)plt.plot(G_losses,label=&quot;G&quot;)plt.plot(D_losses,label=&quot;D&quot;)plt.xlabel(&quot;iterations&quot;)plt.ylabel(&quot;Loss&quot;)plt.legend()plt.show() 接着看一看对于固定输入，生成器的输出随着iteration增加的改变。 12345678#%%captureimport matplotlib.animation as animationfig = plt.figure(figsize=(8,8))plt.axis(&quot;off&quot;)ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)HTML(ani.to_jshtml()) 得到一段动画，截取最开始的输出和最后的输出如下 可以看出生成的人脸有了一定的改善，但效果还是很糟糕。具体的超参数还需要调整。 最后，对比一番真实人脸和生成人脸，可以发现有很大的误差。 123456789101112131415# 真实图片real_batch = next(iter(dataloader))plt.figure(figsize=(15,15))plt.subplot(1,2,1)plt.axis(&quot;off&quot;)plt.title(&quot;Real Images&quot;)plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))# 生成图片plt.subplot(1,2,2)plt.axis(&quot;off&quot;)plt.title(&quot;Fake Images&quot;)plt.imshow(np.transpose(img_list[-1],(1,2,0)))plt.show() 踩过的坑 epoch一开始设置为64，在到达第34个epoch的时候出现问题，输出如下 123456789101112131415161718192021222324252627282930313233343536373839[33/64][0/1583] Loss_D: 0.0413 Loss_G: 5.8435 D(x): 0.9725 D(G(z)): 0.0080 / 0.0126[33/64][100/1583] Loss_D: 0.2360 Loss_G: 3.3545 D(x): 0.8402 D(G(z)): 0.0070 / 0.1269[33/64][200/1583] Loss_D: 0.0288 Loss_G: 6.3076 D(x): 0.9898 D(G(z)): 0.0165 / 0.0065[33/64][300/1583] Loss_D: 0.0714 Loss_G: 5.9589 D(x): 0.9574 D(G(z)): 0.0144 / 0.0119[33/64][400/1583] Loss_D: 0.0526 Loss_G: 5.7749 D(x): 0.9657 D(G(z)): 0.0083 / 0.0146[33/64][500/1583] Loss_D: 0.0645 Loss_G: 7.1439 D(x): 0.9937 D(G(z)): 0.0521 / 0.0021[33/64][600/1583] Loss_D: 0.0893 Loss_G: 7.5577 D(x): 0.9774 D(G(z)): 0.0465 / 0.0026[33/64][700/1583] Loss_D: 0.0616 Loss_G: 7.9219 D(x): 0.9883 D(G(z)): 0.0426 / 0.0023[33/64][800/1583] Loss_D: 0.0697 Loss_G: 6.6769 D(x): 0.9751 D(G(z)): 0.0249 / 0.0049[33/64][900/1583] Loss_D: 0.0596 Loss_G: 5.8319 D(x): 0.9944 D(G(z)): 0.0467 / 0.0078[33/64][1000/1583] Loss_D: 0.2075 Loss_G: 6.8002 D(x): 0.8620 D(G(z)): 0.0072 / 0.0142[33/64][1100/1583] Loss_D: 0.0400 Loss_G: 5.7057 D(x): 0.9850 D(G(z)): 0.0188 / 0.0122[33/64][1200/1583] Loss_D: 0.0311 Loss_G: 7.8273 D(x): 0.9897 D(G(z)): 0.0125 / 0.0023[33/64][1300/1583] Loss_D: 0.0550 Loss_G: 5.3413 D(x): 0.9764 D(G(z)): 0.0252 / 0.0132[33/64][1400/1583] Loss_D: 0.0576 Loss_G: 10.7131 D(x): 0.9515 D(G(z)): 0.0021 / 0.0003[33/64][1500/1583] Loss_D: 0.4995 Loss_G: 10.1190 D(x): 0.9986 D(G(z)): 0.2778 / 0.0002[34/64][0/1583] Loss_D: 0.2651 Loss_G: 6.4838 D(x): 0.8740 D(G(z)): 0.0752 / 0.0063[34/64][100/1583] Loss_D: 0.0746 Loss_G: 7.1531 D(x): 0.9667 D(G(z)): 0.0241 / 0.0043[34/64][200/1583] Loss_D: 0.0262 Loss_G: 6.4364 D(x): 0.9858 D(G(z)): 0.0110 / 0.0055[34/64][300/1583] Loss_D: 0.1436 Loss_G: 5.4750 D(x): 0.9047 D(G(z)): 0.0191 / 0.0280[34/64][400/1583] Loss_D: 0.0332 Loss_G: 6.3144 D(x): 0.9865 D(G(z)): 0.0175 / 0.0111[34/64][500/1583] Loss_D: 0.0632 Loss_G: 9.2094 D(x): 0.9674 D(G(z)): 0.0190 / 0.0005[34/64][600/1583] Loss_D: 0.0000 Loss_G: 67.3240 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000[34/64][700/1583] Loss_D: 0.0001 Loss_G: 66.4839 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000[34/64][800/1583] Loss_D: 0.0000 Loss_G: 66.1828 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000[34/64][900/1583] Loss_D: 0.0000 Loss_G: 66.3180 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000[34/64][1000/1583] Loss_D: 0.0000 Loss_G: 66.2066 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000[34/64][1100/1583] Loss_D: 0.0000 Loss_G: 66.5655 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000[34/64][1200/1583] Loss_D: 0.0000 Loss_G: 67.4126 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000[34/64][1300/1583] Loss_D: 0.0000 Loss_G: 65.4057 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000[34/64][1400/1583] Loss_D: 0.0000 Loss_G: 66.1651 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000[34/64][1500/1583] Loss_D: 0.0000 Loss_G: 66.4172 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000[35/64][0/1583] Loss_D: 0.0000 Loss_G: 66.3567 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000[35/64][100/1583] Loss_D: 0.0000 Loss_G: 66.4605 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000[35/64][200/1583] Loss_D: 0.0000 Loss_G: 65.9061 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000[35/64][300/1583] Loss_D: 0.0000 Loss_G: 65.9909 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000[35/64][400/1583] Loss_D: 0.0000 Loss_G: 65.6323 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000[35/64][500/1583] Loss_D: 0.0000 Loss_G: 66.3318 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000[35/64][600/1583] Loss_D: 0.0000 Loss_G: 66.1391 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000 原因猜测：参数没调好，导致训练走向为判别器越来越强，生成器的生成能力跟不上。 一开始输出就崩了 123456Starting Training Loop...[0/16][0/1344] Loss_D: 2.3426 Loss_G: 17.7673 D(x): 0.2791 D(G(z)): 0.4526 / 0.0000[0/16][100/1344] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000[0/16][200/1344] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000[0/16][300/1344] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000[0/16][400/1344] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000 原因：learning_rate手残把0.0002写成了0.002。导致训练走向为生成器越来越强，判别器判别能力跟不上。 Reference https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html","link":"/2021/09/27/DCGAN/"}],"tags":[{"name":"demo","slug":"demo","link":"/tags/demo/"},{"name":"tutorial","slug":"tutorial","link":"/tags/tutorial/"},{"name":"regex","slug":"regex","link":"/tags/regex/"}],"categories":[]}